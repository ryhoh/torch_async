# -*- coding: utf-8 -*-
"""実験.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MgInoZ0EWa2ZPz8YoIN29X8e5QY2Ap2T

準備
"""

import torch
from torch.autograd import Function
from torch.nn.functional import linear
from torch.nn.modules import Linear

from typing import Union
from math import floor

def percentile(t: torch.tensor, q: float) -> Union[int, float]:
    k = 1 + floor(.01 * float(q) * (t.numel() - 1))
    result = t.view(-1).kthvalue(k).values.item()
    return result

"""Function"""

class Independent_WeightBias_Rate_LinearFunction(Function):
    """独立％のLinearFunction
    更新するニューロンをレイヤ内からn％選ぶ(n=√n ,25, 50, 75), またレイヤ間の従属関係はない
    更新するニューロンは重みとバイアスの勾配から決定する
    勾配の絶対値から大きな値を上位n％個選び更新する
    """  
    @staticmethod
    def forward(ctx, x, w, b=None, rate=100):
        rate = torch.as_tensor(rate).requires_grad_(False)
        ctx.save_for_backward(x, w, b, rate)
        return linear(x, w, b)
    
    @staticmethod
    def backward(ctx,grad_out):
        x, w, b, rate = ctx.saved_tensors
        # 勾配計算
        grad_x = grad_out.mm(w)
        grad_w = grad_out.t().mm(x)
        grad_b = grad_out.sum(0)
        
        if 0 <= rate < 100:
            g_w = grad_w.clone()
            g_b = grad_b.clone()
            g_cat = torch.cat((g_w, g_b.unsqueeze(0).T), 1).abs()
            # 絶対値の上位n％値を計算
            g_rate = percentile(g_cat, 100-rate)
            # マスクを作る
            w_msk = g_w.abs() <= g_rate
            b_msk = g_b.abs() <= g_rate
            # 更新するニューロンを選択
            if torch.sum(w_msk) != w_msk.numel():
                grad_w = grad_w.masked_fill(w_msk, 0)
            else:
                grad_w = torch.zeros(grad_w.shape, dtype=torch.float)
            if torch.sum(b_msk) != b_msk.numel():
                grad_b = grad_b.masked_fill(b_msk, 0)
            else:
                grad_b = torch.zeros(grad_b.shape, dtype=torch.float)
        # 例外処理
        if not ctx.needs_input_grad[0]:
            grad_x = None
        if not ctx.needs_input_grad[1]:
            grad_w = None
        if type(b) != torch.Tensor or not ctx.needs_input_grad[2]:
            grad_b = None
        return grad_x, grad_w, grad_b, None

class Independent_Neuron_Rate_LinearFunction(Function):
    """独立％のLinearFunction
    更新するニューロンをレイヤ内からn％選ぶ(n=√n ,25, 50, 75, 100), またレイヤ間の従属関係はない
    更新するニューロンは重みとバイアスの勾配から決定する
    勾配の絶対値から大きな値を上位n％個選び更新する
    """  
    @staticmethod
    def forward(ctx, x, w, b=None, rate=100):
        rate = torch.as_tensor(rate).requires_grad_(False)
        ctx.save_for_backward(x, w, b, rate)
        return linear(x, w, b)
    
    @staticmethod
    def backward(ctx,grad_out):
        x, w, b, rate = ctx.saved_tensors
        # 勾配計算
        grad_x = grad_out.mm(w)
        grad_w = grad_out.t().mm(x)
        grad_b = grad_out.sum(0)
        # 後で計算に使うマスク(形状がgrad_wで全要素がFalse)
        false_msk = grad_w.abs() < 0

        if 0 <= rate < 100:
            g_w = grad_w.clone()
            g_b = grad_b.clone()
            g_cat = torch.cat((g_w, g_b.unsqueeze(0).T), 1).abs()
            # ニューロンごとに勾配の合計を計算する
            g_sum = torch.sum(g_cat, axis=1)
            # 絶対値の上位n％値を計算
            g_rate = percentile(g_sum, 100-rate)
            # マスクを作る
            b_msk = g_sum <= g_rate
            w_msk = b_msk.unsqueeze(0).T + false_msk
            # 更新するニューロンを選択
            if torch.sum(w_msk) != w_msk.numel():
                grad_w = grad_w.masked_fill(w_msk, 0)
            else:
                grad_w = torch.zeros(grad_w.shape, dtype=torch.float)
            if torch.sum(b_msk) != b_msk.numel():
                grad_b = grad_b.masked_fill(b_msk, 0)
            else:
                grad_b = torch.zeros(grad_b.shape, dtype=torch.float)

        # 例外処理
        if not ctx.needs_input_grad[0]:
            grad_x = None
        if not ctx.needs_input_grad[1]:
            grad_w = None
        if type(b) != torch.Tensor or not ctx.needs_input_grad[2]:
            grad_b = None

        return grad_x, grad_w, grad_b, None

"""Linear"""

class Independent_WeightBias_Rate_Linear(Linear):
    """独立のLinear
    独立更新のレイヤー
    """
    def __init__(self, input_feautures, output_features, bias=True, rate=100):
        super().__init__(input_feautures, output_features, bias)
        self.rate = rate
    def forward(self, input):
        res = Independent_WeightBias_Rate_LinearFunction.apply(input, self.weight, self.bias, self.rate)
        return res

class Independent_Neuron_Rate_Linear(Linear):
    """独立のLinear
    独立更新のレイヤー
    """
    def __init__(self, input_feautures, output_features, bias=True, rate=100):
        super().__init__(input_feautures, output_features, bias)
        self.rate = rate
    def forward(self, input):
        res = Independent_Neuron_Rate_LinearFunction.apply(input, self.weight, self.bias, self.rate)
        return res

"""実験"""

from os import getcwd, makedirs
from os.path import join, isdir
import random
import numpy as np
import torch.backends.cudnn as cudnn
from torch.nn import CrossEntropyLoss
from torch.optim import SGD
# モデル
from torchvision.models import vgg
# データセット
#from preprocess import cifar10_dataloader as cifar10
from procedure.preprocess import cifar10_dataloader as cifar10
# tensorboard
from torch.utils.tensorboard import SummaryWriter
# save list
import pickle
import gc
""" 設定パラメータ"""
model_name = "モデル名"
seed = 0  # ランダムシード
gpu = True  # gpu使用フラグ
lr = 0.001  # 学習率
momentum = 0.9  # モーメンタム
epochs = 50  # Epoch数
batch_size = 100  # バッチサイズ
rate = [1.6, 25, 50, 75, 100][3]
writer = SummaryWriter('runs/' + model_name)

def main():
    global writer, model_name, update_method
    # ランダムシード
    # PyTorch 以外のRNGを初期化
    random.seed(seed)
    np.random.seed(seed)
    # cuDNNを使用しない (遅くなる?)
    cudnn.deterministic = True
    # PyTorchのRNGを初期化
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # デバイス設定
    if torch.cuda.is_available() and gpu is True:
        device = 'cuda'
    else:
        device = 'cpu'
    # モデルを定義
    model = vgg.vgg16()
    # 準同期式に変更
    model.classifier[0] = Independent_Neuron_Rate_Linear(512*7*7, 4096, True, rate)
    model.classifier[3] = Independent_Neuron_Rate_Linear(4096, 4096, True, rate)
    model = model.to(device)
    # 評価関数
    criterion_mean = CrossEntropyLoss().to(device)
    criterion_sum = CrossEntropyLoss(reduction='sum').to(device)

    # パラメータ更新手法
    optimizer = SGD(model.parameters(), lr, momentum=momentum)
    # オートチューナーOFF
    torch.backends.cudnn.benchmark = False
    # Dataset
    train_loader, eval_loader = cifar10(random_seed=seed, batch_size=batch_size)
    # 評価
    evaluate(-1, model, eval_loader, criterion_mean, criterion_sum, device)
    # 学習
    for epoch in range(epochs):
        train(epoch, model, train_loader, optimizer, criterion_mean, criterion_sum, device)
        evaluate(epoch, model, eval_loader, criterion_mean, criterion_sum, device)
    # 学習結果を保存
    save(data=model, name=model_name, task="model")

def mkdirs(path):
    """ ディレクトリが無ければ作る """
    if not isdir(path):
        makedirs(path)

def save(data, name, task):
    """ SAVE MODEL
    data: 保存するデータ
    name: ファイル名
    task: データのタイプ
    """
    global model_name
    save_dir = join(getcwd(), "log/" + model_name)
    mkdirs(save_dir)
    if task == "model":
        """ モデルを保存
        Memo: ロードする方法
        model = TheModelClass(*args, **kwargs)
        model.load_state_dict(torch.load(PATH))
        model.eval()
        """
        torch.save(data.state_dict(), join(save_dir, name+'.model'))
    elif task == "progress":
        """ 予測の途中経過
        Memo: ロードする方法
        data = None
        with open(PATH, 'rb') as f:
            data = pickle.load(f)
        """
        with open(join(save_dir, name+'.dump'), 'wb') as f:
            pickle.dump(data, f)

def evaluate(epoch, model, data_loader, criterion_mean, criterion_sum, device):
    """ 評価用関数 """
    global writer
    model.eval()
    with torch.no_grad():
        answers_list = []
        outputs_list = []
        loss_sum = 0
        accuracy_sum = 0
        item_counter = 0
        for i, (inputs, labels) in enumerate(data_loader):
            # デバイス用設定
            inputs = inputs.to(device)
            labels = labels.to(device)
            # モデルへ適用
            outputs = model(inputs)
            # lossを計算
            # criterion_meanはbackprop/update用
            loss = criterion_mean(outputs, labels)
            # criterion_sumはログ記録用
            loss_sum += criterion_sum(outputs, labels).item()
            # accuracyを計算
            _, argmax = torch.max(outputs, 1)
            accuracy = (labels == argmax.squeeze()).float().sum().item()
            accuracy_sum += accuracy
            # 画像数
            item_counter += len(outputs)
            # log
            answers_list.append(labels.to('cpu'))
            outputs_list.append(outputs.to('cpu'))
            # debug
            print('progress: [{0}/{1}]\t'
                  'Loss: {loss:.3f}\t'
                  'Accuracy: {accuracy:.3f}'.format(
                      i, len(data_loader),
                      loss=loss.item(),
                      accuracy=accuracy/len(outputs)))
        # output log to tensorboard
        writer.add_scalar('evaluate loss',
                          loss_sum/item_counter,
                          epoch)
        writer.add_scalar('evaluate Accuracy',
                          accuracy_sum/item_counter,
                          epoch)
        # save log
        d = {
            "outputs": outputs_list,
            "answers": answers_list
        }
        n = "evaluate{}".format(epoch)
        save(data=d, name=n, task="progress")
        del answers_list
        del outputs_list
        gc.collect()

def train(epoch, model, data_loader, optimizer, criterion_mean, criterion_sum, device):
    """ 学習用関数 """
    global writer
    model.train()
    answers_list = []
    outputs_list = []
    loss_sum = 0
    accuracy_sum = 0
    item_counter = 0
    for i, (inputs, labels) in enumerate(data_loader):
        # デバイス用設定
        inputs = inputs.to(device)
        labels = labels.to(device)
        # 勾配の初期化
        optimizer.zero_grad()
        # モデルへ適用
        outputs = model(inputs)
        # lossを計算
        # criterion_meanはbackprop/update用
        loss = criterion_mean(outputs, labels)
        # criterion_sumはログ記録用
        loss_sum += criterion_sum(outputs, labels).item()
        # accuracyを計算
        _, argmax = torch.max(outputs, 1)
        accuracy = (labels == argmax.squeeze()).float().sum().item()
        accuracy_sum += accuracy
        # 画像数
        item_counter += len(outputs)
        # 逆伝播
        loss.backward()
        # パラメータ更新
        optimizer.step()
        # log
        answers_list.append(labels.to('cpu'))
        outputs_list.append(outputs.to('cpu'))
        # debug
        print('Epoch: [{0}][{1}/{2}]\t'
              'Loss {loss:.4f}\t'
              'Accuracy: {accuracy:.3f}'.format(
               epoch, i, len(data_loader),
               loss=loss.item(),
               accuracy=accuracy/len(outputs)))
    # output log to tensorboard
    writer.add_scalar('train loss',
                      loss_sum/item_counter,
                      epoch)
    writer.add_scalar('train Accuracy',
                      accuracy_sum/item_counter,
                      epoch)
    # save log
    d = {
        "outputs": outputs_list,
        "answers": answers_list
    }
    n = "train_{}".format(epoch)
    save(data=d, name=n, task="progress")
    del answers_list
    del outputs_list
    gc.collect()

if __name__ == '__main__':
    main()

